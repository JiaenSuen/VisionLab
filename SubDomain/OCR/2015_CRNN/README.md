# Applying CRNN to CAPTCHA Recognition
## Introduction
CRNN (Convolutional Recurrent Neural Network) is a deep learning architecture that combines convolutional neural networks (CNN) and recurrent neural networks (RNN) to specifically process data that has spatial structure and sequential characteristics. In computer vision and text recognition tasks, images themselves have spatial features, while text has time series or language sequence properties. Therefore, CRNN provides an end-to-end modeling approach.

Typical application scenarios of CRNN include :
* Scene Text Recognition
* Optical Character Recognition
* Handwritten Character Recognition
* License Plate Recognition

In this project I implemented and demonstrating its application in OCR and CAPTCHA recognition. The entire CRNN architecture and training/testing code framework were built using PyTorch. Using approximately 1000 CAPTCHA images as experimental data, the dataset was split into a 7:3 ratio. A portion of the dataset was used to train a CRNN to minimize the CTC Loss, and the feasibility of this architecture was verified on the other portion of the dataset. Finally, we can see that its model can be trained effectively and significantly.


## Architecture and Technical Details
![image_of_CRNN_Architecture](record_/CRNN_architecture.png)  

CRNN typically contains three main modules  :
### CNN Feature Extractor
This part typically employs convolutional backbone networks such as LeNet, VGG, or others, with the aim of :

* Extracting high-order semantic features from the input image
* Preserving local spatial information
* Reducing dimensionality and improving feature representation capabilities

### RNN Sequence Modeling
The feature maps generated by the CNN are unfolded into a sequence along the width direction and then input into an RNN (commonly LSTM or GRU).
* Bidirectional LSTM (BiLSTM) is a common choice.
* Capable of capturing contextual information
* Solving the problem that a single window cannot understand the meaning of the entire string.

### Transcription Layer
CRNNs are typically paired with CTC (Connectionist Temporal Classification) as the loss function for :
* No character-level alignment annotation required
* Automatically handles inputs and outputs of different lengths
* Solving time alignment issues

### Research Paper Reference , 2015
CRNN (Convolutional Recurrent Neural Network) was originally proposed by Baoguang Shi et al. and published in the paper "An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition". This research aims to solve the problem of recognizing sequential data in images, especially scene text recognition, and proposes a deep neural network architecture that can be trained end-to-end and does not require segmentation.

The core contribution of this paper lies in integrating Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) into a single, unified framework. The CNN is responsible for extracting discriminative high-order visual features from the input image; subsequently, the feature maps are converted into sequence form, and their temporal dependencies are modeled by a Bidirectional Long Short-Term Memory (BiLSTM) network; finally, Connectionist Temporal Classification (CTC) is used as the transcription layer and loss function, enabling the model to complete training without requiring character-by-character alignment annotation. This design effectively overcomes the limitations of traditional OCR systems that rely on character segmentation and manual feature engineering.


Experimental results show that CRNN achieved state-of-the-art performance on multiple public scene text datasets at the time (2015), while also possessing advantages such as relatively simple parameter count, ability to handle variable-length inputs, and generalizability to other image sequence recognition tasks.

## CAPTCHA Dataset for Machine Learning
This dataset contains 1070 images of CAPTCHA, designed to help in training machine learning models for CAPTCHA recognition and solving tasks. CAPTCHA, or "Completely Automated Public Turing test to tell Computers and Humans Apart," is widely used to prevent automated bots from interacting with web services. Recognizing CAPTCHA is a classic machine learning problem that involves image processing, text recognition, and sequence modeling.

https://www.kaggle.com/datasets/mrigaankjaswal/capcha-images-to-training-data

Image Format: PNG
Label Format: Text (corresponding to the CAPTCHA shown in the image)
Average Length of CAPTCHA: Varies (generally 4-6 characters)
CAPTCHA Type: Text-based, containing a mix of alphanumeric characters
Dataset Structure - Each image file is named based on the text it represents (for example, abc123.png for a CAPTCHA showing "abc123")


## Training Method
### Loss Function :  CTC loss
Connectionist Temporal Classification (CTC) is a loss function specifically designed for variable-length sequence recognition, commonly used in tasks such as speech recognition and scene text recognition. Its core purpose is to address the problem of "lack of explicit alignment between the input sequence and the target label," enabling end-to-end training of the model without progressive alignment annotations. The key mechanism of CTC is the introduction of a special blank symbol, allowing for repetition and blanking in the temporal dimension, forming multiple possible alignment paths. Finally, the model sums the probabilities of all valid paths that can be mapped to the target sequence, using this as the conditional probability of the target sequence. This probability and gradient can be efficiently calculated using dynamic programming (forward-backward algorithm). This method avoids the dependence of traditional sequence models on precise temporal alignment, significantly reducing annotation costs while improving the model's resilience and generalization ability in practical applications.

### Optimizer   :   Adam Optimizer
Adam retains Momentume's gradient rate adjust based on the direction of past gradients and Adam's learning rate adjustment based on the squared value of past gradients. In addition, Adam performs parameter "Deviation Correction", ensuring that the learning rate has a defined range for each iteration, resulting in more stable parameter updates.   
Learning Rate : 1e-4
### Reduce LR On Plateau
In this project, I used PyTorch's ReduceLROnPlateau as a learning rate scheduling strategy to dynamically adjust the learning rate during model training. This method monitors a specified validation metric (set to mode='min', typically corresponding to validation loss). When this metric does not significantly decrease within 8 consecutive epochs of patience, the current learning rate is multiplied by factor=0.5, effectively reducing it to half its original value. This mechanism automatically reduces the step size when the model enters a convergence plateau, allowing for more precise parameter updates and preventing oscillations or missing better local minima. Simultaneously, verbose=True outputs prompts during learning rate adjustments, facilitating tracking of the training status. This strategy improves model stability and final convergence quality.

## Validation Metrics


### Character Error Rate / Character Accuracy
In this project, I used the character-level edit distance (Levenshtein distance) as the evaluation basis to measure the model's recognition ability at a fine-grained level. For each predicted character string and the true character string, the minimum edit distance between them (including the number of insertions, deletions, and replacements) was calculated, and the total edit distance of all samples was divided by the total number of characters to obtain the Character Error Rate (CER). This metric reflects the proportion of errors the model makes at the overall character level; the lower the value, the better the recognition quality. Furthermore, I defined Character Accuracy as 1 âˆ’ CER and converted it to a percentage form to make the accuracy rate more intuitive. CER is particularly suitable for analyzing cases of partial recognition errors, such as predictions that are only wrong by one or two characters, and can reflect the actual performance of the model at the character recognition level in detail, rather than being completely zeroed out by errors in the entire sentence.

### OCR Full Accuracy
OCR Full Accuracy employs a strict sentence-by-sentence comparison standard, considering a recognition as correct only when the model's predicted string completely matches the actual string. This metric is calculated by dividing the number of correct samples by the total number of samples to obtain the overall sequence accuracy. Unlike CER, Sequence Accuracy does not allow any character errors, thus better reflecting the model's reliability in practical applications. In real-world OCR systems, even a single character error can lead to semantic errors or information distortion; therefore, this metric can be considered a key measure of system usability. By simultaneously observing CER and OCR Full Accuracy, I can comprehensively evaluate model performance from two perspectives: "fine-grained character quality" and "overall recognition success rate."


## Result
### Training Curve
![image](record_/losses.png)
### Demo Test
![image](record_/crnn_test.png)
### Final Accuracy
|Dataset|CTC Loss|Character Error Rate |Character Accuracy|OCR Full Accuracy |
|-|-|-|-|-| 
|Train|0.0031 |0.0000 | 100.00% |100.00%  |
|Test |0.0425 |0.0100 |  99.00% | 95.02%  |

 